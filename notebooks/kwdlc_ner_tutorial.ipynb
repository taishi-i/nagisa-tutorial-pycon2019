{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kwdlc_ner_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taishi-i/nagisa-tutorial-pycon2019/blob/master/notebooks/kwdlc_ner_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNaehMwWJbz",
        "colab_type": "text"
      },
      "source": [
        "# 1.はじめに\n",
        "京都大学ウェブ文書リードコーパスを利用し、\n",
        "日本語を対象とした固有表現抽出モデルの学習・評価・予測を行います。\n",
        "\n",
        "GPU を利用する場合はランタイム→ランタイムのタイプ変更→GPU をオンにしてください。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzwr_waJ3Oc",
        "colab_type": "text"
      },
      "source": [
        "# 2.事前準備\n",
        "- Python ライブラリーのインストール\n",
        "- 作業ディレクトリの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zGkG2_8lVz2",
        "colab_type": "code",
        "outputId": "133843af-801a-4b01-f436-bf8dccb97cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install bs4\n",
        "!pip install nagisa\n",
        "!pip install seqeval\n",
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: nagisa in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.17.3)\n",
            "Requirement already satisfied: DyNet in /usr/local/lib/python3.6/dist-packages (from nagisa) (2.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from DyNet->nagisa) (0.29.13)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.9.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.1)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.6)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.5)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.11.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.0+cu100)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: transformers>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (2.1.1)\n",
            "Requirement already satisfied: ipython==7.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (7.6.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.1+cu100)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n",
            "Requirement already satisfied: natto-py; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.9.0)\n",
            "Requirement already satisfied: SudachiPy; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.1.83)\n",
            "Requirement already satisfied: kytea; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from tiny-tokenizer[all]->flair) (0.1.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.17.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.4.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (0.0.35)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (2.21.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (4.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.14.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.1)\n",
            "Requirement already satisfied: dartsclone~=0.6.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.6)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.7)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair) (0.46)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bZdPFR1KjGz",
        "colab_type": "code",
        "outputId": "2c7f1c5f-b8bd-4a85-de7a-c452e0ece5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2su_fs9piCz",
        "colab_type": "text"
      },
      "source": [
        "# 3.京都大学ウェブ文書リードコーパスの前処理\n",
        "- GitHub よりコーパスをダウンロードする\n",
        "- nagisa と FRAIR 学習用にスペース区切りのデータセットに変換する\n",
        "- 学習/開発/評価用データセットに分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvuJYhRRkUxO",
        "colab_type": "code",
        "outputId": "18099e7a-b2e8-4e0b-ddfa-fdb774af1993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/ku-nlp/KWDLC"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'KWDLC' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2IvE7h6k7K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import bs4\n",
        "import nagisa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0PflvRnGlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_kwdlc(dir_path):\n",
        "    files = glob.glob(dir_path+\"/*/*\", recursive=True)\n",
        "\n",
        "    data = []\n",
        "    words = []\n",
        "    postgas = []\n",
        "\n",
        "    position2ne = {}\n",
        "\n",
        "    for fn in files:\n",
        "        with open(fn, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                first_char = line[0]\n",
        "\n",
        "                if first_char == \"+\":\n",
        "                    soup = bs4.BeautifulSoup(line, \"html.parser\")\n",
        "                    num_tags = len(soup.contents)\n",
        "                    for i in range(num_tags):\n",
        "                        if str(type(soup.contents[i])) == \"<class 'bs4.element.Tag'>\":\n",
        "                            ne_tag_tokens = str(soup.contents[i]).split(\":\")\n",
        "                            is_ne = ne_tag_tokens[0][1:]\n",
        "\n",
        "                            if is_ne == \"ne\":\n",
        "                                netype = ne_tag_tokens[1]\n",
        "                                target = ne_tag_tokens[2].split(\">\")[0]\n",
        "\n",
        "                                position2ne[len(words)] = [target, netype]\n",
        "\n",
        "                elif first_char == \"#\" or first_char == \"*\":\n",
        "                    None\n",
        "\n",
        "                elif line == \"EOS\":\n",
        "                    # process\n",
        "                    if len(position2ne) > 0:\n",
        "                        positions = position2ne.keys()\n",
        "                        for position in positions:\n",
        "                            target = position2ne[position][0]\n",
        "                            netype = position2ne[position][1]\n",
        "\n",
        "                    data.append([words, postgas, position2ne])\n",
        "\n",
        "                    # reset\n",
        "                    words = []\n",
        "                    postgas = []\n",
        "                    position2ne = {}\n",
        "\n",
        "                else:\n",
        "                    tokens = line.split()\n",
        "                    surface = tokens[0]\n",
        "                    words.append(surface)\n",
        "\n",
        "                    postag = \"_\".join(tokens[3:4])\n",
        "                    postgas.append(postag)\n",
        "\n",
        "    return data, position2ne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczWOYTSLTXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_kwdlc_as_single_file(filename, data, position2ne):\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for line in data:\n",
        "            words, postgas, position2ne = line\n",
        "\n",
        "            nes = [v[0] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            nes = list(reversed(nes))\n",
        "\n",
        "            tags = [v[1] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            tags = list(reversed(tags))\n",
        "\n",
        "            if len(nes) == 0:\n",
        "                None\n",
        "\n",
        "            else:\n",
        "                ne_tags = []\n",
        "\n",
        "                ne = nes.pop()\n",
        "                tag = tags.pop()\n",
        "                ne_target_char = ne[0]\n",
        "\n",
        "                partical = []\n",
        "                for word in words:\n",
        "                    first_char = word[0]\n",
        "                    if first_char == ne_target_char:\n",
        "\n",
        "                        if word in ne:\n",
        "                            partical.append(word)\n",
        "\n",
        "                            if \"\".join(partical) == ne:\n",
        "\n",
        "                                for i, word in enumerate(partical):\n",
        "                                    if i == 0:\n",
        "                                        ne_tags.append(\"B-\"+tag)\n",
        "                                    elif i == len(partical) - 1:\n",
        "                                        ne_tags.append(\"E-\"+tag)\n",
        "                                    else:\n",
        "                                        ne_tags.append(\"M-\"+tag)\n",
        "\n",
        "                                if len(nes) > 0:\n",
        "                                    ne = nes.pop()\n",
        "                                    tag = tags.pop()\n",
        "                                    ne_target_char = ne[0]\n",
        "\n",
        "                                partical = []\n",
        "\n",
        "                            else:\n",
        "                                ne_target_char = ne[len(\"\".join(partical))]\n",
        "\n",
        "                        else:\n",
        "                            partical = []\n",
        "                            ne_tags.append(\"O\")\n",
        "\n",
        "                    else:\n",
        "                        partical = []\n",
        "                        ne_tags.append(\"O\")\n",
        "\n",
        "                for word, postag, ne_tag in zip(words, postgas, ne_tags):\n",
        "                    f.write(\" \".join([word, ne_tag])+\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "\n",
        "def write_file(filename, X, Y):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for x, y in zip(X, Y):\n",
        "            for word, tag in zip(x, y):\n",
        "                f.write(\" \".join([word, tag])+\"\\n\")\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3RUFy0SlU1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load KNP files\n",
        "dir_path = \"KWDLC\"\n",
        "dir_path = os.path.join(dir_path, \"knp\")\n",
        "data, position2ne = load_kwdlc(dir_path)\n",
        "\n",
        "# write a file\n",
        "fn_out = \"data/kwdlc.txt\"\n",
        "write_kwdlc_as_single_file(fn_out, data, position2ne)\n",
        "\n",
        "# divide kwdlc.txt into trainset, devset, testset\n",
        "random.seed(1234)\n",
        "fn_in = \"data/kwdlc.txt\"\n",
        "fn_out_train = \"data/kwdlc.train\"\n",
        "fn_out_dev = \"data/kwdlc.dev\"\n",
        "fn_out_test = \"data/kwdlc.test\"\n",
        "train_data = 0.9\n",
        "dev_data = 0.05\n",
        "test_data = 0.05\n",
        "\n",
        "X, Y = nagisa.utils.load_file(fn_in, delimiter=' ', newline='')                                    \n",
        "indice = [i for i in range(len(X))]                                        \n",
        "random.shuffle(indice)                                                     \n",
        "                                                                           \n",
        "num_train = int(train_data * len(indice))                                  \n",
        "num_dev = int(dev_data * len(indice))                                      \n",
        "num_test = int(test_data * len(indice))                                    \n",
        "                                                                           \n",
        "train_X = [X[i] for i in indice[:num_train]]                               \n",
        "train_Y = [Y[i] for i in indice[:num_train]]                               \n",
        "write_file(fn_out_train, train_X, train_Y)                                 \n",
        "                                                                           \n",
        "dev_X = [X[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "dev_Y = [Y[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "write_file(fn_out_dev, dev_X, dev_Y)                                       \n",
        "                                                                           \n",
        "test_X = [X[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "test_Y = [Y[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "write_file(fn_out_test, test_X, test_Y)                                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBdZM6jqMmN",
        "colab_type": "text"
      },
      "source": [
        "# 4.固有表現抽出モデルの学習 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKQLz1KOqPdV",
        "colab_type": "code",
        "outputId": "39ab8d2d-6d5f-4cbb-9266-6b35764eba3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "nagisa.fit(\n",
        "    train_file=\"data/kwdlc.train\",\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\",\n",
        "    model_name=\"data/kwdlc_ner_model\",\n",
        "    delimiter=' ',  # delimiter=\"\\t\"\n",
        "    newline='',  # newline='EOS'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nagisa] LAYERS: 1\n",
            "[nagisa] THRESHOLD: 3\n",
            "[nagisa] DECAY: 1\n",
            "[nagisa] EPOCH: 10\n",
            "[nagisa] WINDOW_SIZE: 3\n",
            "[nagisa] DIM_UNI: 32\n",
            "[nagisa] DIM_BI: 16\n",
            "[nagisa] DIM_WORD: 16\n",
            "[nagisa] DIM_CTYPE: 8\n",
            "[nagisa] DIM_TAGEMB: 16\n",
            "[nagisa] DIM_HIDDEN: 100\n",
            "[nagisa] LEARNING_RATE: 0.1\n",
            "[nagisa] DROPOUT_RATE: 0.3\n",
            "[nagisa] SEED: 1234\n",
            "[nagisa] TRAINSET: data/kwdlc.train\n",
            "[nagisa] TESTSET: data/kwdlc.test\n",
            "[nagisa] DEVSET: data/kwdlc.dev\n",
            "[nagisa] DICTIONARY: None\n",
            "[nagisa] EMBEDDING: None\n",
            "[nagisa] HYPERPARAMS: data/kwdlc_ner_model.hp\n",
            "[nagisa] MODEL: data/kwdlc_ner_model.params\n",
            "[nagisa] VOCAB: data/kwdlc_ner_model.vocabs\n",
            "[nagisa] EPOCH_MODEL: data/kwdlc_ner_model_epoch.params\n",
            "[nagisa] NUM_TRAIN: 4642\n",
            "[nagisa] NUM_TEST: 257\n",
            "[nagisa] NUM_DEV: 257\n",
            "[nagisa] VOCAB_SIZE_UNI: 1927\n",
            "[nagisa] VOCAB_SIZE_BI: 15055\n",
            "[nagisa] VOCAB_SIZE_WORD: 5638\n",
            "[nagisa] VOCAB_SIZE_POSTAG: 29\n",
            "Epoch\tLR   \tLoss \tTime_m\tDevWS_f1\tDevPOS_f1\tTestWS_f1\tTestPOS_f1\n",
            "1    \t0.100\t14.21\t1.308\t92.95   \t84.11   \t91.96   \t83.16   \n",
            "2    \t0.100\t8.399\t1.326\t93.70   \t86.35   \t93.79   \t86.65   \n",
            "3    \t0.100\t6.584\t1.248\t93.91   \t86.41   \t93.70   \t85.74   \n",
            "4    \t0.100\t5.557\t1.262\t94.58   \t87.81   \t93.98   \t86.92   \n",
            "5    \t0.100\t4.806\t1.237\t94.62   \t87.59   \t94.03   \t88.14   \n",
            "6    \t0.100\t4.238\t1.245\t94.63   \t87.89   \t94.49   \t88.39   \n",
            "7    \t0.050\t3.710\t1.213\t94.54   \t87.52   \t94.49   \t88.39   \n",
            "8    \t0.050\t2.843\t1.228\t94.96   \t88.30   \t94.77   \t88.86   \n",
            "9    \t0.025\t2.461\t1.216\t94.95   \t88.73   \t94.77   \t88.86   \n",
            "10   \t0.025\t1.993\t1.235\t95.43   \t88.78   \t94.64   \t88.63   \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjugcRirN0hW",
        "colab_type": "text"
      },
      "source": [
        "# 5.固有表現抽出モデルの評価 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DbeQuHYOOsi",
        "colab_type": "code",
        "outputId": "4fcdca85-e58b-43c5-d104-286e9f7b9232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs='data/kwdlc_ner_model.vocabs',\n",
        "    params='data/kwdlc_ner_model.params',\n",
        "    hp='data/kwdlc_ner_model.hp'\n",
        ")\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline='')\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    pred_y = ner_tagger.decode(x)\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.35      0.37      0.36        46\n",
            "        date       0.82      0.91      0.86        86\n",
            "        time       0.62      0.50      0.56        10\n",
            "    location       0.70      0.75      0.73       132\n",
            "organization       0.47      0.46      0.47        54\n",
            "      person       0.49      0.60      0.54        58\n",
            "    optional       0.20      0.13      0.16        15\n",
            "       money       0.38      1.00      0.55         3\n",
            "     percent       0.67      0.67      0.67         3\n",
            "\n",
            "   micro avg       0.61      0.65      0.63       407\n",
            "   macro avg       0.60      0.65      0.63       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0iW6MIPdsN",
        "colab_type": "text"
      },
      "source": [
        "# 6.固有表現抽出モデルの予測 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_slHwWzGPpLb",
        "colab_type": "code",
        "outputId": "e55df6d7-1517-4e32-9601-fe63480b4d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs=\"data/kwdlc_ner_model.vocabs\",\n",
        "    params=\"data/kwdlc_ner_model.params\",\n",
        "    hp=\"data/kwdlc_ner_model.hp\"\n",
        ")\n",
        "\n",
        "text = \"FacebookのAIラボ所長でもあるヤン・ルカン博士\"\n",
        "tokens = ner_tagger.tagging(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Facebook/O の/O AI/O ラボ/E-person 所長/O で/O も/O ある/O ヤン/B-person ・/M-person ルカン/E-person 博士/O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYeUt-0rQBH0",
        "colab_type": "text"
      },
      "source": [
        "# 7.固有表現抽出モデルの学習 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafqHh1bQYJp",
        "colab_type": "code",
        "outputId": "64c730d2-6f11-416a-d2ab-8cc9984a99b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "# preprocess \n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = '.'\n",
        "corpus = ColumnCorpus(\n",
        "    data_folder,\n",
        "    columns,\n",
        "    train_file='data/kwdlc.train',\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\"\n",
        ")\n",
        "\n",
        "tag_type = 'ner'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "# construct a flair model\n",
        "embedding_types = [\n",
        "    FlairEmbeddings('ja-forward'),\n",
        "    FlairEmbeddings('ja-backward'),\n",
        "]\n",
        "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "tagger = SequenceTagger(\n",
        "    hidden_size=256,\n",
        "    embeddings=embeddings,\n",
        "    tag_dictionary=tag_dictionary,\n",
        "    tag_type=tag_type,\n",
        "    use_crf=True\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "trainer.train(\n",
        "    'resources/taggers/example-ner',\n",
        "    learning_rate=0.1,\n",
        "    mini_batch_size=32,\n",
        "    max_epochs=5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-06 23:51:15,253 Reading data from .\n",
            "2019-11-06 23:51:15,254 Train: data/kwdlc.train\n",
            "2019-11-06 23:51:15,256 Dev: data/kwdlc.dev\n",
            "2019-11-06 23:51:15,258 Test: data/kwdlc.test\n",
            "2019-11-06 23:51:19,291 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,297 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (encoder): Embedding(15174, 100)\n",
            "        (rnn): LSTM(100, 2048, num_layers=2, dropout=0.3)\n",
            "        (decoder): Linear(in_features=2048, out_features=15174, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (encoder): Embedding(15174, 100)\n",
            "        (rnn): LSTM(100, 2048, num_layers=2, dropout=0.3)\n",
            "        (decoder): Linear(in_features=2048, out_features=15174, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
            ")\"\n",
            "2019-11-06 23:51:19,298 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,299 Corpus: \"Corpus: 4642 train + 257 dev + 257 test sentences\"\n",
            "2019-11-06 23:51:19,301 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,302 Parameters:\n",
            "2019-11-06 23:51:19,303  - learning_rate: \"0.1\"\n",
            "2019-11-06 23:51:19,305  - mini_batch_size: \"32\"\n",
            "2019-11-06 23:51:19,306  - patience: \"3\"\n",
            "2019-11-06 23:51:19,307  - anneal_factor: \"0.5\"\n",
            "2019-11-06 23:51:19,308  - max_epochs: \"5\"\n",
            "2019-11-06 23:51:19,310  - shuffle: \"True\"\n",
            "2019-11-06 23:51:19,311  - train_with_dev: \"False\"\n",
            "2019-11-06 23:51:19,312  - batch_growth_annealing: \"False\"\n",
            "2019-11-06 23:51:19,313 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,315 Model training base path: \"resources/taggers/example-ner\"\n",
            "2019-11-06 23:51:19,316 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,317 Device: cuda:0\n",
            "2019-11-06 23:51:19,319 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:19,320 Embeddings storage mode: cpu\n",
            "2019-11-06 23:51:19,322 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:51:20,696 epoch 1 - iter 0/146 - loss 67.73997498 - samples/sec: 326.66\n",
            "2019-11-06 23:51:36,748 epoch 1 - iter 14/146 - loss 27.15688445 - samples/sec: 27.99\n",
            "2019-11-06 23:51:52,205 epoch 1 - iter 28/146 - loss 21.17346520 - samples/sec: 29.05\n",
            "2019-11-06 23:52:10,506 epoch 1 - iter 42/146 - loss 18.74591918 - samples/sec: 24.53\n",
            "2019-11-06 23:52:27,320 epoch 1 - iter 56/146 - loss 16.82734617 - samples/sec: 26.70\n",
            "2019-11-06 23:52:43,759 epoch 1 - iter 70/146 - loss 15.50747339 - samples/sec: 27.31\n",
            "2019-11-06 23:53:01,623 epoch 1 - iter 84/146 - loss 14.40217633 - samples/sec: 25.13\n",
            "2019-11-06 23:53:17,537 epoch 1 - iter 98/146 - loss 13.40709997 - samples/sec: 28.23\n",
            "2019-11-06 23:53:34,406 epoch 1 - iter 112/146 - loss 12.68116910 - samples/sec: 26.61\n",
            "2019-11-06 23:53:50,976 epoch 1 - iter 126/146 - loss 12.06187588 - samples/sec: 27.09\n",
            "2019-11-06 23:54:08,620 epoch 1 - iter 140/146 - loss 11.51058387 - samples/sec: 25.45\n",
            "2019-11-06 23:54:14,329 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:54:14,330 EPOCH 1 done: loss 11.2945 - lr 0.1000\n",
            "2019-11-06 23:54:23,495 DEV : loss 5.63138484954834 - score 0.6041\n",
            "2019-11-06 23:54:23,515 BAD EPOCHS (no improvement): 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-06 23:54:26,360 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:54:26,651 epoch 2 - iter 0/146 - loss 7.69033527 - samples/sec: 1593.76\n",
            "2019-11-06 23:54:30,520 epoch 2 - iter 14/146 - loss 5.98698533 - samples/sec: 117.00\n",
            "2019-11-06 23:54:34,531 epoch 2 - iter 28/146 - loss 6.01677123 - samples/sec: 112.82\n",
            "2019-11-06 23:54:38,100 epoch 2 - iter 42/146 - loss 5.75156003 - samples/sec: 126.85\n",
            "2019-11-06 23:54:41,629 epoch 2 - iter 56/146 - loss 5.60207131 - samples/sec: 128.31\n",
            "2019-11-06 23:54:45,038 epoch 2 - iter 70/146 - loss 5.45444450 - samples/sec: 133.20\n",
            "2019-11-06 23:54:48,556 epoch 2 - iter 84/146 - loss 5.30247875 - samples/sec: 128.68\n",
            "2019-11-06 23:54:51,918 epoch 2 - iter 98/146 - loss 5.17066354 - samples/sec: 135.15\n",
            "2019-11-06 23:54:55,223 epoch 2 - iter 112/146 - loss 5.06472963 - samples/sec: 137.24\n",
            "2019-11-06 23:54:58,726 epoch 2 - iter 126/146 - loss 4.96589064 - samples/sec: 129.31\n",
            "2019-11-06 23:55:02,170 epoch 2 - iter 140/146 - loss 4.93809595 - samples/sec: 131.44\n",
            "2019-11-06 23:55:03,305 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:55:03,307 EPOCH 2 done: loss 4.8873 - lr 0.1000\n",
            "2019-11-06 23:55:04,209 DEV : loss 3.3508732318878174 - score 0.7612\n",
            "2019-11-06 23:55:04,229 BAD EPOCHS (no improvement): 0\n",
            "2019-11-06 23:55:07,046 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:55:07,308 epoch 3 - iter 0/146 - loss 5.44387627 - samples/sec: 1860.99\n",
            "2019-11-06 23:55:10,816 epoch 3 - iter 14/146 - loss 3.96448636 - samples/sec: 129.28\n",
            "2019-11-06 23:55:14,764 epoch 3 - iter 28/146 - loss 3.74311781 - samples/sec: 114.87\n",
            "2019-11-06 23:55:18,301 epoch 3 - iter 42/146 - loss 3.79019051 - samples/sec: 128.06\n",
            "2019-11-06 23:55:21,901 epoch 3 - iter 56/146 - loss 3.77465955 - samples/sec: 125.72\n",
            "2019-11-06 23:55:25,525 epoch 3 - iter 70/146 - loss 3.84162647 - samples/sec: 124.90\n",
            "2019-11-06 23:55:29,440 epoch 3 - iter 84/146 - loss 3.85452711 - samples/sec: 116.15\n",
            "2019-11-06 23:55:32,965 epoch 3 - iter 98/146 - loss 3.86430066 - samples/sec: 128.50\n",
            "2019-11-06 23:55:36,770 epoch 3 - iter 112/146 - loss 3.84065324 - samples/sec: 118.93\n",
            "2019-11-06 23:55:40,363 epoch 3 - iter 126/146 - loss 3.81295149 - samples/sec: 126.02\n",
            "2019-11-06 23:55:43,742 epoch 3 - iter 140/146 - loss 3.79323453 - samples/sec: 134.12\n",
            "2019-11-06 23:55:44,843 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:55:44,844 EPOCH 3 done: loss 3.8126 - lr 0.1000\n",
            "2019-11-06 23:55:45,922 DEV : loss 3.2216129302978516 - score 0.7667\n",
            "2019-11-06 23:55:45,942 BAD EPOCHS (no improvement): 0\n",
            "2019-11-06 23:55:48,680 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:55:48,993 epoch 4 - iter 0/146 - loss 4.70230198 - samples/sec: 1443.12\n",
            "2019-11-06 23:55:53,057 epoch 4 - iter 14/146 - loss 3.77162530 - samples/sec: 111.66\n",
            "2019-11-06 23:55:56,601 epoch 4 - iter 28/146 - loss 3.44708940 - samples/sec: 128.20\n",
            "2019-11-06 23:56:00,102 epoch 4 - iter 42/146 - loss 3.40597432 - samples/sec: 129.26\n",
            "2019-11-06 23:56:03,617 epoch 4 - iter 56/146 - loss 3.33658191 - samples/sec: 128.82\n",
            "2019-11-06 23:56:07,061 epoch 4 - iter 70/146 - loss 3.33611424 - samples/sec: 131.59\n",
            "2019-11-06 23:56:10,866 epoch 4 - iter 84/146 - loss 3.30218311 - samples/sec: 119.22\n",
            "2019-11-06 23:56:14,395 epoch 4 - iter 98/146 - loss 3.26286107 - samples/sec: 128.41\n",
            "2019-11-06 23:56:17,994 epoch 4 - iter 112/146 - loss 3.24665053 - samples/sec: 125.72\n",
            "2019-11-06 23:56:21,626 epoch 4 - iter 126/146 - loss 3.19227927 - samples/sec: 124.59\n",
            "2019-11-06 23:56:25,201 epoch 4 - iter 140/146 - loss 3.18212216 - samples/sec: 126.69\n",
            "2019-11-06 23:56:26,283 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:56:26,284 EPOCH 4 done: loss 3.1704 - lr 0.1000\n",
            "2019-11-06 23:56:27,407 DEV : loss 2.7882626056671143 - score 0.7835\n",
            "2019-11-06 23:56:27,429 BAD EPOCHS (no improvement): 0\n",
            "2019-11-06 23:56:30,231 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:56:30,547 epoch 5 - iter 0/146 - loss 2.11380291 - samples/sec: 1737.67\n",
            "2019-11-06 23:56:34,477 epoch 5 - iter 14/146 - loss 3.10478708 - samples/sec: 115.02\n",
            "2019-11-06 23:56:38,033 epoch 5 - iter 28/146 - loss 2.97318143 - samples/sec: 127.68\n",
            "2019-11-06 23:56:41,634 epoch 5 - iter 42/146 - loss 2.92934210 - samples/sec: 125.65\n",
            "2019-11-06 23:56:45,196 epoch 5 - iter 56/146 - loss 2.95927724 - samples/sec: 127.14\n",
            "2019-11-06 23:56:48,952 epoch 5 - iter 70/146 - loss 2.89600999 - samples/sec: 120.49\n",
            "2019-11-06 23:56:52,578 epoch 5 - iter 84/146 - loss 2.84549617 - samples/sec: 124.77\n",
            "2019-11-06 23:56:56,294 epoch 5 - iter 98/146 - loss 2.82068383 - samples/sec: 121.87\n",
            "2019-11-06 23:56:59,820 epoch 5 - iter 112/146 - loss 2.82800833 - samples/sec: 128.43\n",
            "2019-11-06 23:57:03,389 epoch 5 - iter 126/146 - loss 2.80668031 - samples/sec: 126.78\n",
            "2019-11-06 23:57:06,746 epoch 5 - iter 140/146 - loss 2.79267791 - samples/sec: 134.91\n",
            "2019-11-06 23:57:07,856 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:57:07,857 EPOCH 5 done: loss 2.7953 - lr 0.1000\n",
            "2019-11-06 23:57:08,808 DEV : loss 2.6015942096710205 - score 0.77\n",
            "2019-11-06 23:57:08,829 BAD EPOCHS (no improvement): 1\n",
            "2019-11-06 23:57:11,732 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-06 23:57:11,740 Testing using best model ...\n",
            "2019-11-06 23:57:11,745 loading file resources/taggers/example-ner/best-model.pt\n",
            "2019-11-06 23:57:21,405 0.8879\t0.6719\t0.7649\n",
            "2019-11-06 23:57:21,407 \n",
            "MICRO_AVG: acc 0.6193 - f1-score 0.7649\n",
            "MACRO_AVG: acc 0.4639 - f1-score 0.5528529411764707\n",
            "M-artifact tp: 12 - fp: 0 - fn: 22 - tn: 12 - precision: 1.0000 - recall: 0.3529 - accuracy: 0.3529 - f1-score: 0.5217\n",
            "M-date     tp: 73 - fp: 4 - fn: 4 - tn: 73 - precision: 0.9481 - recall: 0.9481 - accuracy: 0.9012 - f1-score: 0.9481\n",
            "M-location tp: 9 - fp: 0 - fn: 1 - tn: 9 - precision: 1.0000 - recall: 0.9000 - accuracy: 0.9000 - f1-score: 0.9474\n",
            "M-optional tp: 0 - fp: 0 - fn: 5 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "M-organization tp: 16 - fp: 2 - fn: 20 - tn: 16 - precision: 0.8889 - recall: 0.4444 - accuracy: 0.4211 - f1-score: 0.5926\n",
            "M-percent  tp: 0 - fp: 0 - fn: 2 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "M-person   tp: 17 - fp: 0 - fn: 4 - tn: 17 - precision: 1.0000 - recall: 0.8095 - accuracy: 0.8095 - f1-score: 0.8947\n",
            "M-time     tp: 1 - fp: 0 - fn: 7 - tn: 1 - precision: 1.0000 - recall: 0.1250 - accuracy: 0.1250 - f1-score: 0.2222\n",
            "artifact   tp: 19 - fp: 0 - fn: 47 - tn: 19 - precision: 1.0000 - recall: 0.2879 - accuracy: 0.2879 - f1-score: 0.4471\n",
            "date       tp: 110 - fp: 10 - fn: 12 - tn: 110 - precision: 0.9167 - recall: 0.9016 - accuracy: 0.8333 - f1-score: 0.9091\n",
            "location   tp: 115 - fp: 20 - fn: 23 - tn: 115 - precision: 0.8519 - recall: 0.8333 - accuracy: 0.7278 - f1-score: 0.8425\n",
            "money      tp: 3 - fp: 0 - fn: 0 - tn: 3 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "optional   tp: 0 - fp: 0 - fn: 19 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "organization tp: 38 - fp: 14 - fn: 38 - tn: 38 - precision: 0.7308 - recall: 0.5000 - accuracy: 0.4222 - f1-score: 0.5938\n",
            "percent    tp: 1 - fp: 2 - fn: 3 - tn: 1 - precision: 0.3333 - recall: 0.2500 - accuracy: 0.1667 - f1-score: 0.2857\n",
            "person     tp: 58 - fp: 7 - fn: 15 - tn: 58 - precision: 0.8923 - recall: 0.7945 - accuracy: 0.7250 - f1-score: 0.8406\n",
            "time       tp: 3 - fp: 1 - fn: 10 - tn: 3 - precision: 0.7500 - recall: 0.2308 - accuracy: 0.2143 - f1-score: 0.3530\n",
            "2019-11-06 23:57:21,408 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.7649,\n",
              " 'dev_score_history': [0.6041, 0.7612, 0.7667, 0.7835, 0.77],\n",
              " 'train_loss_history': [11.294457814464831,\n",
              "  4.887346600833005,\n",
              "  3.812639979467,\n",
              "  3.1704277004281134,\n",
              "  2.7953055884740126],\n",
              " 'dev_loss_history': [tensor(5.6314, device='cuda:0'),\n",
              "  tensor(3.3509, device='cuda:0'),\n",
              "  tensor(3.2216, device='cuda:0'),\n",
              "  tensor(2.7883, device='cuda:0'),\n",
              "  tensor(2.6016, device='cuda:0')]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lY8N28PVGj4",
        "colab_type": "text"
      },
      "source": [
        "# 8.固有表現抽出モデルの評価 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LsvsnR1ZUdv",
        "colab_type": "code",
        "outputId": "846b7551-d5f6-4656-bce6-0e35ea52c420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline=\"\")\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    text = \" \".join(x)\n",
        "    sentence = Sentence(text)\n",
        "\n",
        "    model.predict(sentence)\n",
        "    tagged_text = sentence.to_tagged_string()\n",
        "\n",
        "    tokens = tagged_text.split()\n",
        "\n",
        "    words = []\n",
        "    tags = []\n",
        "    for token in tokens:\n",
        "        first_char = token[0]\n",
        "        last_char = token[-1]\n",
        "\n",
        "        if first_char == \"<\" and last_char == \">\":\n",
        "            tag = token[1:-1]\n",
        "            tags[-1] = tag\n",
        "        else:\n",
        "            words.append(token)\n",
        "            tags.append(\"O\")\n",
        "\n",
        "    pred_y = tags\n",
        "\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-06 23:57:21,441 loading file resources/taggers/example-ner/final-model.pt\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.60      0.57      0.58        46\n",
            "        date       0.91      0.90      0.90        86\n",
            "        time       0.00      0.00      0.00        10\n",
            "    location       0.87      0.83      0.85       132\n",
            "organization       0.69      0.50      0.58        54\n",
            "      person       0.75      0.74      0.75        58\n",
            "    optional       0.00      0.00      0.00        15\n",
            "       money       0.60      1.00      0.75         3\n",
            "     percent       0.50      0.67      0.57         3\n",
            "\n",
            "   micro avg       0.80      0.71      0.75       407\n",
            "   macro avg       0.75      0.71      0.73       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoViwfoaVk7W",
        "colab_type": "text"
      },
      "source": [
        "# 9.固有表現抽出モデルの予測 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Oo5XRiV-F3",
        "colab_type": "code",
        "outputId": "00a0aad8-c453-4389-82ca-5a5fa3be7f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "                                                                                                                                                    \n",
        "text = \"Facebook の AI ラボ 所長 でも ある ヤン ・ ルカン 博士\"         \n",
        "sentence = Sentence(text)                                               \n",
        "model.predict(sentence)                                                 \n",
        "print(sentence.to_tagged_string())  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-06 23:57:54,873 loading file resources/taggers/example-ner/final-model.pt\n",
            "Facebook の AI ラボ 所長 でも ある ヤン <B-person> ・ <M-person> ルカン <E-person> 博士 <E-person>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}