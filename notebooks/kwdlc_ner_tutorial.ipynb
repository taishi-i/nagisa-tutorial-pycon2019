{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kwdlc_ner_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taishi-i/nagisa-tutorial-pycon2019/blob/master/notebooks/kwdlc_ner_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNaehMwWJbz",
        "colab_type": "text"
      },
      "source": [
        "# 1.はじめに\n",
        "京都大学ウェブ文書リードコーパスを利用し、\n",
        "日本語を対象とした固有表現抽出モデルの学習・評価・予測を行います。\n",
        "\n",
        "GPU を利用する場合は **ランタイム→ランタイムのタイプ変更→GPU** をオンにしてください。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzwr_waJ3Oc",
        "colab_type": "text"
      },
      "source": [
        "# 2.事前準備\n",
        "- Python ライブラリーのインストール\n",
        "- 作業ディレクトリの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zGkG2_8lVz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bs4\n",
        "!pip install nagisa\n",
        "!pip install seqeval\n",
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bZdPFR1KjGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2su_fs9piCz",
        "colab_type": "text"
      },
      "source": [
        "# 3.京都大学ウェブ文書リードコーパスの前処理\n",
        "- GitHub よりコーパスをダウンロードする\n",
        "- nagisa と FLAIR 学習用にスペース区切りのデータセットに変換する\n",
        "- 学習/開発/評価用データセットに分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvuJYhRRkUxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ku-nlp/KWDLC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2IvE7h6k7K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import bs4\n",
        "import nagisa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0PflvRnGlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_kwdlc(dir_path):\n",
        "    files = glob.glob(dir_path+\"/*/*\", recursive=True)\n",
        "\n",
        "    data = []\n",
        "    words = []\n",
        "    postgas = []\n",
        "\n",
        "    position2ne = {}\n",
        "\n",
        "    for fn in files:\n",
        "        with open(fn, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                first_char = line[0]\n",
        "\n",
        "                if first_char == \"+\":\n",
        "                    soup = bs4.BeautifulSoup(line, \"html.parser\")\n",
        "                    num_tags = len(soup.contents)\n",
        "                    for i in range(num_tags):\n",
        "                        if str(type(soup.contents[i])) == \"<class 'bs4.element.Tag'>\":\n",
        "                            ne_tag_tokens = str(soup.contents[i]).split(\":\")\n",
        "                            is_ne = ne_tag_tokens[0][1:]\n",
        "\n",
        "                            if is_ne == \"ne\":\n",
        "                                netype = ne_tag_tokens[1]\n",
        "                                target = ne_tag_tokens[2].split(\">\")[0]\n",
        "\n",
        "                                position2ne[len(words)] = [target, netype]\n",
        "\n",
        "                elif first_char == \"#\" or first_char == \"*\":\n",
        "                    None\n",
        "\n",
        "                elif line == \"EOS\":\n",
        "                    # process\n",
        "                    if len(position2ne) > 0:\n",
        "                        positions = position2ne.keys()\n",
        "                        for position in positions:\n",
        "                            target = position2ne[position][0]\n",
        "                            netype = position2ne[position][1]\n",
        "\n",
        "                    data.append([words, postgas, position2ne])\n",
        "\n",
        "                    # reset\n",
        "                    words = []\n",
        "                    postgas = []\n",
        "                    position2ne = {}\n",
        "\n",
        "                else:\n",
        "                    tokens = line.split()\n",
        "                    surface = tokens[0]\n",
        "                    words.append(surface)\n",
        "\n",
        "                    postag = \"_\".join(tokens[3:4])\n",
        "                    postgas.append(postag)\n",
        "\n",
        "    return data, position2ne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczWOYTSLTXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_kwdlc_as_single_file(filename, data, position2ne):\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for line in data:\n",
        "            words, postgas, position2ne = line\n",
        "\n",
        "            nes = [v[0] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            nes = list(reversed(nes))\n",
        "\n",
        "            tags = [v[1] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            tags = list(reversed(tags))\n",
        "\n",
        "            if len(nes) == 0:\n",
        "                None\n",
        "\n",
        "            else:\n",
        "                ne_tags = []\n",
        "\n",
        "                ne = nes.pop()\n",
        "                tag = tags.pop()\n",
        "                ne_target_char = ne[0]\n",
        "\n",
        "                partical = []\n",
        "                for word in words:\n",
        "                    first_char = word[0]\n",
        "                    if first_char == ne_target_char:\n",
        "\n",
        "                        if word in ne:\n",
        "                            partical.append(word)\n",
        "\n",
        "                            if \"\".join(partical) == ne:\n",
        "\n",
        "                                for i, word in enumerate(partical):\n",
        "                                    if i == 0:\n",
        "                                        ne_tags.append(\"B-\"+tag)\n",
        "                                    elif i == len(partical) - 1:\n",
        "                                        ne_tags.append(\"E-\"+tag)\n",
        "                                    else:\n",
        "                                        ne_tags.append(\"M-\"+tag)\n",
        "\n",
        "                                if len(nes) > 0:\n",
        "                                    ne = nes.pop()\n",
        "                                    tag = tags.pop()\n",
        "                                    ne_target_char = ne[0]\n",
        "\n",
        "                                partical = []\n",
        "\n",
        "                            else:\n",
        "                                ne_target_char = ne[len(\"\".join(partical))]\n",
        "\n",
        "                        else:\n",
        "                            partical = []\n",
        "                            ne_tags.append(\"O\")\n",
        "\n",
        "                    else:\n",
        "                        partical = []\n",
        "                        ne_tags.append(\"O\")\n",
        "\n",
        "                for word, postag, ne_tag in zip(words, postgas, ne_tags):\n",
        "                    f.write(\" \".join([word, ne_tag])+\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "\n",
        "def write_file(filename, X, Y):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for x, y in zip(X, Y):\n",
        "            for word, tag in zip(x, y):\n",
        "                f.write(\" \".join([word, tag])+\"\\n\")\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3RUFy0SlU1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load KNP files\n",
        "dir_path = \"KWDLC\"\n",
        "dir_path = os.path.join(dir_path, \"knp\")\n",
        "data, position2ne = load_kwdlc(dir_path)\n",
        "\n",
        "# write a file\n",
        "fn_out = \"data/kwdlc.txt\"\n",
        "write_kwdlc_as_single_file(fn_out, data, position2ne)\n",
        "\n",
        "# divide kwdlc.txt into trainset, devset, testset\n",
        "random.seed(1234)\n",
        "fn_in = \"data/kwdlc.txt\"\n",
        "fn_out_train = \"data/kwdlc.train\"\n",
        "fn_out_dev = \"data/kwdlc.dev\"\n",
        "fn_out_test = \"data/kwdlc.test\"\n",
        "train_data = 0.9\n",
        "dev_data = 0.05\n",
        "test_data = 0.05\n",
        "\n",
        "X, Y = nagisa.utils.load_file(fn_in, delimiter=' ', newline='')                                    \n",
        "indice = [i for i in range(len(X))]                                        \n",
        "random.shuffle(indice)                                                     \n",
        "                                                                           \n",
        "num_train = int(train_data * len(indice))                                  \n",
        "num_dev = int(dev_data * len(indice))                                      \n",
        "num_test = int(test_data * len(indice))                                    \n",
        "                                                                           \n",
        "train_X = [X[i] for i in indice[:num_train]]                               \n",
        "train_Y = [Y[i] for i in indice[:num_train]]                               \n",
        "write_file(fn_out_train, train_X, train_Y)                                 \n",
        "                                                                           \n",
        "dev_X = [X[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "dev_Y = [Y[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "write_file(fn_out_dev, dev_X, dev_Y)                                       \n",
        "                                                                           \n",
        "test_X = [X[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "test_Y = [Y[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "write_file(fn_out_test, test_X, test_Y)                                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBdZM6jqMmN",
        "colab_type": "text"
      },
      "source": [
        "# 4.固有表現抽出モデルの学習 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKQLz1KOqPdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nagisa.fit(\n",
        "    train_file=\"data/kwdlc.train\",\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\",\n",
        "    model_name=\"data/kwdlc_ner_model\",\n",
        "    delimiter=' ',  # delimiter=\"\\t\"\n",
        "    newline='',  # newline='EOS'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjugcRirN0hW",
        "colab_type": "text"
      },
      "source": [
        "# 5.固有表現抽出モデルの評価 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DbeQuHYOOsi",
        "colab_type": "code",
        "outputId": "4fcdca85-e58b-43c5-d104-286e9f7b9232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs='data/kwdlc_ner_model.vocabs',\n",
        "    params='data/kwdlc_ner_model.params',\n",
        "    hp='data/kwdlc_ner_model.hp'\n",
        ")\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline='')\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    pred_y = ner_tagger.decode(x)\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.35      0.37      0.36        46\n",
            "        date       0.82      0.91      0.86        86\n",
            "        time       0.62      0.50      0.56        10\n",
            "    location       0.70      0.75      0.73       132\n",
            "organization       0.47      0.46      0.47        54\n",
            "      person       0.49      0.60      0.54        58\n",
            "    optional       0.20      0.13      0.16        15\n",
            "       money       0.38      1.00      0.55         3\n",
            "     percent       0.67      0.67      0.67         3\n",
            "\n",
            "   micro avg       0.61      0.65      0.63       407\n",
            "   macro avg       0.60      0.65      0.63       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0iW6MIPdsN",
        "colab_type": "text"
      },
      "source": [
        "# 6.固有表現抽出モデルの予測 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_slHwWzGPpLb",
        "colab_type": "code",
        "outputId": "e55df6d7-1517-4e32-9601-fe63480b4d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs=\"data/kwdlc_ner_model.vocabs\",\n",
        "    params=\"data/kwdlc_ner_model.params\",\n",
        "    hp=\"data/kwdlc_ner_model.hp\"\n",
        ")\n",
        "\n",
        "text = \"FacebookのAIラボ所長でもあるヤン・ルカン博士\"\n",
        "tokens = ner_tagger.tagging(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Facebook/O の/O AI/O ラボ/E-person 所長/O で/O も/O ある/O ヤン/B-person ・/M-person ルカン/E-person 博士/O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYeUt-0rQBH0",
        "colab_type": "text"
      },
      "source": [
        "# 7.固有表現抽出モデルの学習 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafqHh1bQYJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "# preprocess \n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = '.'\n",
        "corpus = ColumnCorpus(\n",
        "    data_folder,\n",
        "    columns,\n",
        "    train_file='data/kwdlc.train',\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\"\n",
        ")\n",
        "\n",
        "tag_type = 'ner'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "# construct a flair model\n",
        "embedding_types = [\n",
        "    FlairEmbeddings('ja-forward'),\n",
        "    FlairEmbeddings('ja-backward'),\n",
        "]\n",
        "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "tagger = SequenceTagger(\n",
        "    hidden_size=256,\n",
        "    embeddings=embeddings,\n",
        "    tag_dictionary=tag_dictionary,\n",
        "    tag_type=tag_type,\n",
        "    use_crf=True\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "trainer.train(\n",
        "    'resources/taggers/example-ner',\n",
        "    learning_rate=0.1,\n",
        "    mini_batch_size=32,\n",
        "    max_epochs=10\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lY8N28PVGj4",
        "colab_type": "text"
      },
      "source": [
        "# 8.固有表現抽出モデルの評価 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LsvsnR1ZUdv",
        "colab_type": "code",
        "outputId": "a2a24494-0191-47e0-f2f3-f171d6cb980c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline=\"\")\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    text = \" \".join(x)\n",
        "    sentence = Sentence(text)\n",
        "\n",
        "    model.predict(sentence)\n",
        "    tagged_text = sentence.to_tagged_string()\n",
        "\n",
        "    tokens = tagged_text.split()\n",
        "\n",
        "    words = []\n",
        "    tags = []\n",
        "    for token in tokens:\n",
        "        first_char = token[0]\n",
        "        last_char = token[-1]\n",
        "\n",
        "        if first_char == \"<\" and last_char == \">\":\n",
        "            tag = token[1:-1]\n",
        "            tags[-1] = tag\n",
        "        else:\n",
        "            words.append(token)\n",
        "            tags.append(\"O\")\n",
        "\n",
        "    pred_y = tags\n",
        "\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:30:16,051 loading file resources/taggers/example-ner/final-model.pt\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.64      0.61      0.62        46\n",
            "        date       0.87      0.94      0.91        86\n",
            "        time       0.44      0.70      0.54        10\n",
            "    location       0.89      0.87      0.88       132\n",
            "organization       0.71      0.59      0.65        54\n",
            "      person       0.85      0.79      0.82        58\n",
            "    optional       0.36      0.27      0.31        15\n",
            "       money       0.75      1.00      0.86         3\n",
            "     percent       0.25      0.33      0.29         3\n",
            "\n",
            "   micro avg       0.79      0.78      0.79       407\n",
            "   macro avg       0.79      0.78      0.78       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoViwfoaVk7W",
        "colab_type": "text"
      },
      "source": [
        "# 9.固有表現抽出モデルの予測 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Oo5XRiV-F3",
        "colab_type": "code",
        "outputId": "a61350f1-db11-4ad7-c64e-25745177514d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "                                                                                                                                                    \n",
        "text = \"Facebook の AI ラボ 所長 でも ある ヤン ・ ルカン 博士\"         \n",
        "sentence = Sentence(text)                                               \n",
        "model.predict(sentence)                                                 \n",
        "print(sentence.to_tagged_string())  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:30:56,475 loading file resources/taggers/example-ner/final-model.pt\n",
            "Facebook の AI ラボ 所長 でも ある ヤン <B-person> ・ <M-person> ルカン <E-person> 博士\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}