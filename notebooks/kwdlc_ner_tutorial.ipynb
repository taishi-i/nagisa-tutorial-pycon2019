{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kwdlc_ner_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taishi-i/nagisa-tutorial-pycon2019/blob/master/notebooks/kwdlc_ner_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNaehMwWJbz",
        "colab_type": "text"
      },
      "source": [
        "# 1.はじめに\n",
        "京都大学ウェブ文書リードコーパスを利用し、\n",
        "日本語を対象とした固有表現抽出モデルの学習・評価・予測を行います。\n",
        "\n",
        "GPU を利用する場合はランタイム→ランタイムのタイプ変更→GPU をオンにしてください。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzwr_waJ3Oc",
        "colab_type": "text"
      },
      "source": [
        "# 2.事前準備\n",
        "- Python ライブラリーのインストール\n",
        "- 作業ディレクトリの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zGkG2_8lVz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bs4\n",
        "!pip install nagisa\n",
        "!pip install seqeval\n",
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bZdPFR1KjGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2su_fs9piCz",
        "colab_type": "text"
      },
      "source": [
        "# 3.京都大学ウェブ文書リードコーパスの前処理\n",
        "- GitHub よりコーパスをダウンロードする\n",
        "- nagisa と FRAIR 学習用にスペース区切りのデータセットに変換する\n",
        "- 学習/開発/評価用データセットに分割する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvuJYhRRkUxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ku-nlp/KWDLC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2IvE7h6k7K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import bs4\n",
        "import nagisa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0PflvRnGlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_kwdlc(dir_path):\n",
        "    files = glob.glob(dir_path+\"/*/*\", recursive=True)\n",
        "\n",
        "    data = []\n",
        "    words = []\n",
        "    postgas = []\n",
        "\n",
        "    position2ne = {}\n",
        "\n",
        "    for fn in files:\n",
        "        with open(fn, \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                first_char = line[0]\n",
        "\n",
        "                if first_char == \"+\":\n",
        "                    soup = bs4.BeautifulSoup(line, \"html.parser\")\n",
        "                    num_tags = len(soup.contents)\n",
        "                    for i in range(num_tags):\n",
        "                        if str(type(soup.contents[i])) == \"<class 'bs4.element.Tag'>\":\n",
        "                            ne_tag_tokens = str(soup.contents[i]).split(\":\")\n",
        "                            is_ne = ne_tag_tokens[0][1:]\n",
        "\n",
        "                            if is_ne == \"ne\":\n",
        "                                netype = ne_tag_tokens[1]\n",
        "                                target = ne_tag_tokens[2].split(\">\")[0]\n",
        "\n",
        "                                position2ne[len(words)] = [target, netype]\n",
        "\n",
        "                elif first_char == \"#\" or first_char == \"*\":\n",
        "                    None\n",
        "\n",
        "                elif line == \"EOS\":\n",
        "                    # process\n",
        "                    if len(position2ne) > 0:\n",
        "                        positions = position2ne.keys()\n",
        "                        for position in positions:\n",
        "                            target = position2ne[position][0]\n",
        "                            netype = position2ne[position][1]\n",
        "\n",
        "                    data.append([words, postgas, position2ne])\n",
        "\n",
        "                    # reset\n",
        "                    words = []\n",
        "                    postgas = []\n",
        "                    position2ne = {}\n",
        "\n",
        "                else:\n",
        "                    tokens = line.split()\n",
        "                    surface = tokens[0]\n",
        "                    words.append(surface)\n",
        "\n",
        "                    postag = \"_\".join(tokens[3:4])\n",
        "                    postgas.append(postag)\n",
        "\n",
        "    return data, position2ne"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczWOYTSLTXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_kwdlc_as_single_file(filename, data, position2ne):\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for line in data:\n",
        "            words, postgas, position2ne = line\n",
        "\n",
        "            nes = [v[0] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            nes = list(reversed(nes))\n",
        "\n",
        "            tags = [v[1] for k, v in sorted(position2ne.items(), key=lambda x:x[0])]\n",
        "            tags = list(reversed(tags))\n",
        "\n",
        "            if len(nes) == 0:\n",
        "                None\n",
        "\n",
        "            else:\n",
        "                ne_tags = []\n",
        "\n",
        "                ne = nes.pop()\n",
        "                tag = tags.pop()\n",
        "                ne_target_char = ne[0]\n",
        "\n",
        "                partical = []\n",
        "                for word in words:\n",
        "                    first_char = word[0]\n",
        "                    if first_char == ne_target_char:\n",
        "\n",
        "                        if word in ne:\n",
        "                            partical.append(word)\n",
        "\n",
        "                            if \"\".join(partical) == ne:\n",
        "\n",
        "                                for i, word in enumerate(partical):\n",
        "                                    if i == 0:\n",
        "                                        ne_tags.append(\"B-\"+tag)\n",
        "                                    elif i == len(partical) - 1:\n",
        "                                        ne_tags.append(\"E-\"+tag)\n",
        "                                    else:\n",
        "                                        ne_tags.append(\"M-\"+tag)\n",
        "\n",
        "                                if len(nes) > 0:\n",
        "                                    ne = nes.pop()\n",
        "                                    tag = tags.pop()\n",
        "                                    ne_target_char = ne[0]\n",
        "\n",
        "                                partical = []\n",
        "\n",
        "                            else:\n",
        "                                ne_target_char = ne[len(\"\".join(partical))]\n",
        "\n",
        "                        else:\n",
        "                            partical = []\n",
        "                            ne_tags.append(\"O\")\n",
        "\n",
        "                    else:\n",
        "                        partical = []\n",
        "                        ne_tags.append(\"O\")\n",
        "\n",
        "                for word, postag, ne_tag in zip(words, postgas, ne_tags):\n",
        "                    f.write(\" \".join([word, ne_tag])+\"\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "\n",
        "def write_file(filename, X, Y):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for x, y in zip(X, Y):\n",
        "            for word, tag in zip(x, y):\n",
        "                f.write(\" \".join([word, tag])+\"\\n\")\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3RUFy0SlU1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load KNP files\n",
        "dir_path = \"KWDLC\"\n",
        "dir_path = os.path.join(dir_path, \"knp\")\n",
        "data, position2ne = load_kwdlc(dir_path)\n",
        "\n",
        "# write a file\n",
        "fn_out = \"data/kwdlc.txt\"\n",
        "write_kwdlc_as_single_file(fn_out, data, position2ne)\n",
        "\n",
        "# divide kwdlc.txt into trainset, devset, testset\n",
        "random.seed(1234)\n",
        "fn_in = \"data/kwdlc.txt\"\n",
        "fn_out_train = \"data/kwdlc.train\"\n",
        "fn_out_dev = \"data/kwdlc.dev\"\n",
        "fn_out_test = \"data/kwdlc.test\"\n",
        "train_data = 0.9\n",
        "dev_data = 0.05\n",
        "test_data = 0.05\n",
        "\n",
        "X, Y = nagisa.utils.load_file(fn_in, delimiter=' ', newline='')                                    \n",
        "indice = [i for i in range(len(X))]                                        \n",
        "random.shuffle(indice)                                                     \n",
        "                                                                           \n",
        "num_train = int(train_data * len(indice))                                  \n",
        "num_dev = int(dev_data * len(indice))                                      \n",
        "num_test = int(test_data * len(indice))                                    \n",
        "                                                                           \n",
        "train_X = [X[i] for i in indice[:num_train]]                               \n",
        "train_Y = [Y[i] for i in indice[:num_train]]                               \n",
        "write_file(fn_out_train, train_X, train_Y)                                 \n",
        "                                                                           \n",
        "dev_X = [X[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "dev_Y = [Y[i] for i in indice[num_train:num_train+num_dev]]                \n",
        "write_file(fn_out_dev, dev_X, dev_Y)                                       \n",
        "                                                                           \n",
        "test_X = [X[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "test_Y = [Y[i] for i in indice[num_train+num_dev:num_train+num_dev+num_test]]\n",
        "write_file(fn_out_test, test_X, test_Y)                                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBdZM6jqMmN",
        "colab_type": "text"
      },
      "source": [
        "# 4.固有表現抽出モデルの学習 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKQLz1KOqPdV",
        "colab_type": "code",
        "outputId": "39ab8d2d-6d5f-4cbb-9266-6b35764eba3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "nagisa.fit(\n",
        "    train_file=\"data/kwdlc.train\",\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\",\n",
        "    model_name=\"data/kwdlc_ner_model\",\n",
        "    delimiter=' ',  # delimiter=\"\\t\"\n",
        "    newline='',  # newline='EOS'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nagisa] LAYERS: 1\n",
            "[nagisa] THRESHOLD: 3\n",
            "[nagisa] DECAY: 1\n",
            "[nagisa] EPOCH: 10\n",
            "[nagisa] WINDOW_SIZE: 3\n",
            "[nagisa] DIM_UNI: 32\n",
            "[nagisa] DIM_BI: 16\n",
            "[nagisa] DIM_WORD: 16\n",
            "[nagisa] DIM_CTYPE: 8\n",
            "[nagisa] DIM_TAGEMB: 16\n",
            "[nagisa] DIM_HIDDEN: 100\n",
            "[nagisa] LEARNING_RATE: 0.1\n",
            "[nagisa] DROPOUT_RATE: 0.3\n",
            "[nagisa] SEED: 1234\n",
            "[nagisa] TRAINSET: data/kwdlc.train\n",
            "[nagisa] TESTSET: data/kwdlc.test\n",
            "[nagisa] DEVSET: data/kwdlc.dev\n",
            "[nagisa] DICTIONARY: None\n",
            "[nagisa] EMBEDDING: None\n",
            "[nagisa] HYPERPARAMS: data/kwdlc_ner_model.hp\n",
            "[nagisa] MODEL: data/kwdlc_ner_model.params\n",
            "[nagisa] VOCAB: data/kwdlc_ner_model.vocabs\n",
            "[nagisa] EPOCH_MODEL: data/kwdlc_ner_model_epoch.params\n",
            "[nagisa] NUM_TRAIN: 4642\n",
            "[nagisa] NUM_TEST: 257\n",
            "[nagisa] NUM_DEV: 257\n",
            "[nagisa] VOCAB_SIZE_UNI: 1927\n",
            "[nagisa] VOCAB_SIZE_BI: 15055\n",
            "[nagisa] VOCAB_SIZE_WORD: 5638\n",
            "[nagisa] VOCAB_SIZE_POSTAG: 29\n",
            "Epoch\tLR   \tLoss \tTime_m\tDevWS_f1\tDevPOS_f1\tTestWS_f1\tTestPOS_f1\n",
            "1    \t0.100\t14.21\t1.308\t92.95   \t84.11   \t91.96   \t83.16   \n",
            "2    \t0.100\t8.399\t1.326\t93.70   \t86.35   \t93.79   \t86.65   \n",
            "3    \t0.100\t6.584\t1.248\t93.91   \t86.41   \t93.70   \t85.74   \n",
            "4    \t0.100\t5.557\t1.262\t94.58   \t87.81   \t93.98   \t86.92   \n",
            "5    \t0.100\t4.806\t1.237\t94.62   \t87.59   \t94.03   \t88.14   \n",
            "6    \t0.100\t4.238\t1.245\t94.63   \t87.89   \t94.49   \t88.39   \n",
            "7    \t0.050\t3.710\t1.213\t94.54   \t87.52   \t94.49   \t88.39   \n",
            "8    \t0.050\t2.843\t1.228\t94.96   \t88.30   \t94.77   \t88.86   \n",
            "9    \t0.025\t2.461\t1.216\t94.95   \t88.73   \t94.77   \t88.86   \n",
            "10   \t0.025\t1.993\t1.235\t95.43   \t88.78   \t94.64   \t88.63   \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjugcRirN0hW",
        "colab_type": "text"
      },
      "source": [
        "# 5.固有表現抽出モデルの評価 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DbeQuHYOOsi",
        "colab_type": "code",
        "outputId": "4fcdca85-e58b-43c5-d104-286e9f7b9232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs='data/kwdlc_ner_model.vocabs',\n",
        "    params='data/kwdlc_ner_model.params',\n",
        "    hp='data/kwdlc_ner_model.hp'\n",
        ")\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline='')\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    pred_y = ner_tagger.decode(x)\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.35      0.37      0.36        46\n",
            "        date       0.82      0.91      0.86        86\n",
            "        time       0.62      0.50      0.56        10\n",
            "    location       0.70      0.75      0.73       132\n",
            "organization       0.47      0.46      0.47        54\n",
            "      person       0.49      0.60      0.54        58\n",
            "    optional       0.20      0.13      0.16        15\n",
            "       money       0.38      1.00      0.55         3\n",
            "     percent       0.67      0.67      0.67         3\n",
            "\n",
            "   micro avg       0.61      0.65      0.63       407\n",
            "   macro avg       0.60      0.65      0.63       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0iW6MIPdsN",
        "colab_type": "text"
      },
      "source": [
        "# 6.固有表現抽出モデルの予測 (nagisa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_slHwWzGPpLb",
        "colab_type": "code",
        "outputId": "e55df6d7-1517-4e32-9601-fe63480b4d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ner_tagger = nagisa.Tagger(\n",
        "    vocabs=\"data/kwdlc_ner_model.vocabs\",\n",
        "    params=\"data/kwdlc_ner_model.params\",\n",
        "    hp=\"data/kwdlc_ner_model.hp\"\n",
        ")\n",
        "\n",
        "text = \"FacebookのAIラボ所長でもあるヤン・ルカン博士\"\n",
        "tokens = ner_tagger.tagging(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Facebook/O の/O AI/O ラボ/E-person 所長/O で/O も/O ある/O ヤン/B-person ・/M-person ルカン/E-person 博士/O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYeUt-0rQBH0",
        "colab_type": "text"
      },
      "source": [
        "# 7.固有表現抽出モデルの学習 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafqHh1bQYJp",
        "colab_type": "code",
        "outputId": "bb3d99f2-20d7-485d-8c9c-ca39a7dc81ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "from flair.embeddings import StackedEmbeddings\n",
        "\n",
        "# preprocess \n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "data_folder = '.'\n",
        "corpus = ColumnCorpus(\n",
        "    data_folder,\n",
        "    columns,\n",
        "    train_file='data/kwdlc.train',\n",
        "    dev_file=\"data/kwdlc.dev\",\n",
        "    test_file=\"data/kwdlc.test\"\n",
        ")\n",
        "\n",
        "tag_type = 'ner'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "# construct a flair model\n",
        "embedding_types = [\n",
        "    FlairEmbeddings('ja-forward'),\n",
        "    FlairEmbeddings('ja-backward'),\n",
        "]\n",
        "embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "tagger = SequenceTagger(\n",
        "    hidden_size=256,\n",
        "    embeddings=embeddings,\n",
        "    tag_dictionary=tag_dictionary,\n",
        "    tag_type=tag_type,\n",
        "    use_crf=True\n",
        ")\n",
        "\n",
        "# start training\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "trainer.train(\n",
        "    'resources/taggers/example-ner',\n",
        "    learning_rate=0.1,\n",
        "    mini_batch_size=32,\n",
        "    max_epochs=10\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:20:04,811 Reading data from .\n",
            "2019-11-07 00:20:04,813 Train: data/kwdlc.train\n",
            "2019-11-07 00:20:04,819 Dev: data/kwdlc.dev\n",
            "2019-11-07 00:20:04,825 Test: data/kwdlc.test\n",
            "2019-11-07 00:20:09,696 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,698 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (encoder): Embedding(15174, 100)\n",
            "        (rnn): LSTM(100, 2048, num_layers=2, dropout=0.3)\n",
            "        (decoder): Linear(in_features=2048, out_features=15174, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (encoder): Embedding(15174, 100)\n",
            "        (rnn): LSTM(100, 2048, num_layers=2, dropout=0.3)\n",
            "        (decoder): Linear(in_features=2048, out_features=15174, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=31, bias=True)\n",
            ")\"\n",
            "2019-11-07 00:20:09,700 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,701 Corpus: \"Corpus: 4642 train + 257 dev + 257 test sentences\"\n",
            "2019-11-07 00:20:09,703 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,704 Parameters:\n",
            "2019-11-07 00:20:09,706  - learning_rate: \"0.1\"\n",
            "2019-11-07 00:20:09,707  - mini_batch_size: \"32\"\n",
            "2019-11-07 00:20:09,709  - patience: \"3\"\n",
            "2019-11-07 00:20:09,710  - anneal_factor: \"0.5\"\n",
            "2019-11-07 00:20:09,711  - max_epochs: \"10\"\n",
            "2019-11-07 00:20:09,712  - shuffle: \"True\"\n",
            "2019-11-07 00:20:09,713  - train_with_dev: \"False\"\n",
            "2019-11-07 00:20:09,715  - batch_growth_annealing: \"False\"\n",
            "2019-11-07 00:20:09,716 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,717 Model training base path: \"resources/taggers/example-ner\"\n",
            "2019-11-07 00:20:09,718 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,719 Device: cuda:0\n",
            "2019-11-07 00:20:09,721 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:09,722 Embeddings storage mode: cpu\n",
            "2019-11-07 00:20:09,723 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:20:11,017 epoch 1 - iter 0/146 - loss 68.28334045 - samples/sec: 347.12\n",
            "2019-11-07 00:20:28,516 epoch 1 - iter 14/146 - loss 27.86938883 - samples/sec: 25.67\n",
            "2019-11-07 00:20:45,306 epoch 1 - iter 28/146 - loss 22.10244330 - samples/sec: 26.76\n",
            "2019-11-07 00:21:02,836 epoch 1 - iter 42/146 - loss 19.72238729 - samples/sec: 25.63\n",
            "2019-11-07 00:21:19,734 epoch 1 - iter 56/146 - loss 18.03922175 - samples/sec: 26.58\n",
            "2019-11-07 00:21:36,543 epoch 1 - iter 70/146 - loss 16.45972232 - samples/sec: 26.73\n",
            "2019-11-07 00:21:53,579 epoch 1 - iter 84/146 - loss 15.27066657 - samples/sec: 26.36\n",
            "2019-11-07 00:22:11,375 epoch 1 - iter 98/146 - loss 14.25334436 - samples/sec: 25.23\n",
            "2019-11-07 00:22:27,807 epoch 1 - iter 112/146 - loss 13.36110618 - samples/sec: 27.35\n",
            "2019-11-07 00:22:44,480 epoch 1 - iter 126/146 - loss 12.58128025 - samples/sec: 26.95\n",
            "2019-11-07 00:23:01,471 epoch 1 - iter 140/146 - loss 11.95368853 - samples/sec: 26.42\n",
            "2019-11-07 00:23:06,325 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:23:06,327 EPOCH 1 done: loss 11.7811 - lr 0.1000\n",
            "2019-11-07 00:23:15,246 DEV : loss 5.676308631896973 - score 0.6969\n",
            "2019-11-07 00:23:15,271 BAD EPOCHS (no improvement): 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:23:17,975 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:23:18,443 epoch 2 - iter 0/146 - loss 5.52695751 - samples/sec: 963.81\n",
            "2019-11-07 00:23:22,350 epoch 2 - iter 14/146 - loss 5.65929953 - samples/sec: 115.77\n",
            "2019-11-07 00:23:26,066 epoch 2 - iter 28/146 - loss 5.68580937 - samples/sec: 122.23\n",
            "2019-11-07 00:23:29,920 epoch 2 - iter 42/146 - loss 5.66411025 - samples/sec: 117.48\n",
            "2019-11-07 00:23:33,679 epoch 2 - iter 56/146 - loss 5.58606237 - samples/sec: 120.51\n",
            "2019-11-07 00:23:37,154 epoch 2 - iter 70/146 - loss 5.41597591 - samples/sec: 130.48\n",
            "2019-11-07 00:23:40,804 epoch 2 - iter 84/146 - loss 5.43181476 - samples/sec: 124.17\n",
            "2019-11-07 00:23:44,388 epoch 2 - iter 98/146 - loss 5.31911413 - samples/sec: 126.48\n",
            "2019-11-07 00:23:48,405 epoch 2 - iter 112/146 - loss 5.16790419 - samples/sec: 112.77\n",
            "2019-11-07 00:23:51,978 epoch 2 - iter 126/146 - loss 5.10893094 - samples/sec: 126.77\n",
            "2019-11-07 00:23:55,701 epoch 2 - iter 140/146 - loss 5.06735605 - samples/sec: 121.65\n",
            "2019-11-07 00:23:56,826 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:23:56,827 EPOCH 2 done: loss 5.0517 - lr 0.1000\n",
            "2019-11-07 00:23:57,912 DEV : loss 3.9821128845214844 - score 0.7686\n",
            "2019-11-07 00:23:57,934 BAD EPOCHS (no improvement): 0\n",
            "2019-11-07 00:24:00,725 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:24:01,048 epoch 3 - iter 0/146 - loss 4.52162361 - samples/sec: 1420.79\n",
            "2019-11-07 00:24:04,697 epoch 3 - iter 14/146 - loss 4.25714882 - samples/sec: 123.99\n",
            "2019-11-07 00:24:08,434 epoch 3 - iter 28/146 - loss 4.26999417 - samples/sec: 121.23\n",
            "2019-11-07 00:24:12,293 epoch 3 - iter 42/146 - loss 4.16728111 - samples/sec: 117.32\n",
            "2019-11-07 00:24:15,903 epoch 3 - iter 56/146 - loss 4.06964345 - samples/sec: 125.86\n",
            "2019-11-07 00:24:19,770 epoch 3 - iter 70/146 - loss 4.04613929 - samples/sec: 117.08\n",
            "2019-11-07 00:24:23,335 epoch 3 - iter 84/146 - loss 4.05364921 - samples/sec: 127.14\n",
            "2019-11-07 00:24:27,066 epoch 3 - iter 98/146 - loss 4.06902346 - samples/sec: 121.63\n",
            "2019-11-07 00:24:30,667 epoch 3 - iter 112/146 - loss 4.00025451 - samples/sec: 125.81\n",
            "2019-11-07 00:24:34,248 epoch 3 - iter 126/146 - loss 3.95952190 - samples/sec: 126.63\n",
            "2019-11-07 00:24:37,886 epoch 3 - iter 140/146 - loss 3.95202980 - samples/sec: 124.54\n",
            "2019-11-07 00:24:39,085 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:24:39,087 EPOCH 3 done: loss 3.9208 - lr 0.1000\n",
            "2019-11-07 00:24:40,109 DEV : loss 2.9843506813049316 - score 0.7586\n",
            "2019-11-07 00:24:40,136 BAD EPOCHS (no improvement): 1\n",
            "2019-11-07 00:24:40,137 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:24:40,373 epoch 4 - iter 0/146 - loss 2.49549723 - samples/sec: 1922.79\n",
            "2019-11-07 00:24:43,877 epoch 4 - iter 14/146 - loss 3.29868025 - samples/sec: 129.24\n",
            "2019-11-07 00:24:47,361 epoch 4 - iter 28/146 - loss 3.43799425 - samples/sec: 130.16\n",
            "2019-11-07 00:24:51,164 epoch 4 - iter 42/146 - loss 3.45275914 - samples/sec: 119.43\n",
            "2019-11-07 00:24:54,942 epoch 4 - iter 56/146 - loss 3.37932934 - samples/sec: 120.15\n",
            "2019-11-07 00:24:58,354 epoch 4 - iter 70/146 - loss 3.37264320 - samples/sec: 132.85\n",
            "2019-11-07 00:25:01,982 epoch 4 - iter 84/146 - loss 3.35062266 - samples/sec: 124.89\n",
            "2019-11-07 00:25:05,442 epoch 4 - iter 98/146 - loss 3.32628508 - samples/sec: 130.91\n",
            "2019-11-07 00:25:09,222 epoch 4 - iter 112/146 - loss 3.31787760 - samples/sec: 119.75\n",
            "2019-11-07 00:25:12,693 epoch 4 - iter 126/146 - loss 3.32247058 - samples/sec: 130.63\n",
            "2019-11-07 00:25:16,174 epoch 4 - iter 140/146 - loss 3.35013122 - samples/sec: 130.25\n",
            "2019-11-07 00:25:17,364 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:25:17,365 EPOCH 4 done: loss 3.3376 - lr 0.1000\n",
            "2019-11-07 00:25:18,289 DEV : loss 2.717393159866333 - score 0.7531\n",
            "2019-11-07 00:25:18,311 BAD EPOCHS (no improvement): 2\n",
            "2019-11-07 00:25:18,313 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:25:18,757 epoch 5 - iter 0/146 - loss 3.63560534 - samples/sec: 1015.90\n",
            "2019-11-07 00:25:22,630 epoch 5 - iter 14/146 - loss 2.98431614 - samples/sec: 116.87\n",
            "2019-11-07 00:25:26,428 epoch 5 - iter 28/146 - loss 2.94843231 - samples/sec: 119.34\n",
            "2019-11-07 00:25:30,087 epoch 5 - iter 42/146 - loss 2.92928015 - samples/sec: 124.12\n",
            "2019-11-07 00:25:33,695 epoch 5 - iter 56/146 - loss 2.98896058 - samples/sec: 125.56\n",
            "2019-11-07 00:25:37,316 epoch 5 - iter 70/146 - loss 2.96425252 - samples/sec: 125.27\n",
            "2019-11-07 00:25:40,882 epoch 5 - iter 84/146 - loss 2.96058063 - samples/sec: 127.05\n",
            "2019-11-07 00:25:44,318 epoch 5 - iter 98/146 - loss 2.94773422 - samples/sec: 132.16\n",
            "2019-11-07 00:25:47,716 epoch 5 - iter 112/146 - loss 2.92248249 - samples/sec: 133.44\n",
            "2019-11-07 00:25:51,277 epoch 5 - iter 126/146 - loss 2.93430684 - samples/sec: 127.29\n",
            "2019-11-07 00:25:54,889 epoch 5 - iter 140/146 - loss 2.92375803 - samples/sec: 125.38\n",
            "2019-11-07 00:25:55,981 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:25:55,983 EPOCH 5 done: loss 2.9302 - lr 0.1000\n",
            "2019-11-07 00:25:56,922 DEV : loss 2.527315855026245 - score 0.7721\n",
            "2019-11-07 00:25:56,948 BAD EPOCHS (no improvement): 0\n",
            "2019-11-07 00:25:59,691 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:25:59,989 epoch 6 - iter 0/146 - loss 2.55399823 - samples/sec: 1518.79\n",
            "2019-11-07 00:26:03,966 epoch 6 - iter 14/146 - loss 2.57918138 - samples/sec: 114.10\n",
            "2019-11-07 00:26:07,889 epoch 6 - iter 28/146 - loss 2.46223380 - samples/sec: 115.54\n",
            "2019-11-07 00:26:11,630 epoch 6 - iter 42/146 - loss 2.46623458 - samples/sec: 121.11\n",
            "2019-11-07 00:26:15,265 epoch 6 - iter 56/146 - loss 2.59930023 - samples/sec: 124.57\n",
            "2019-11-07 00:26:18,925 epoch 6 - iter 70/146 - loss 2.60346770 - samples/sec: 123.77\n",
            "2019-11-07 00:26:22,430 epoch 6 - iter 84/146 - loss 2.60678823 - samples/sec: 129.34\n",
            "2019-11-07 00:26:26,210 epoch 6 - iter 98/146 - loss 2.61469242 - samples/sec: 119.74\n",
            "2019-11-07 00:26:29,593 epoch 6 - iter 112/146 - loss 2.60859556 - samples/sec: 134.22\n",
            "2019-11-07 00:26:33,017 epoch 6 - iter 126/146 - loss 2.60419793 - samples/sec: 132.52\n",
            "2019-11-07 00:26:36,691 epoch 6 - iter 140/146 - loss 2.62809815 - samples/sec: 123.36\n",
            "2019-11-07 00:26:37,865 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:26:37,866 EPOCH 6 done: loss 2.6283 - lr 0.1000\n",
            "2019-11-07 00:26:38,802 DEV : loss 2.4380571842193604 - score 0.8007\n",
            "2019-11-07 00:26:38,823 BAD EPOCHS (no improvement): 0\n",
            "2019-11-07 00:26:41,635 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:26:42,034 epoch 7 - iter 0/146 - loss 2.43200302 - samples/sec: 1331.94\n",
            "2019-11-07 00:26:45,601 epoch 7 - iter 14/146 - loss 2.31813474 - samples/sec: 133.98\n",
            "2019-11-07 00:26:49,204 epoch 7 - iter 28/146 - loss 2.30910879 - samples/sec: 125.62\n",
            "2019-11-07 00:26:52,891 epoch 7 - iter 42/146 - loss 2.42110636 - samples/sec: 122.92\n",
            "2019-11-07 00:26:56,447 epoch 7 - iter 56/146 - loss 2.40123434 - samples/sec: 127.41\n",
            "2019-11-07 00:27:00,097 epoch 7 - iter 70/146 - loss 2.39333667 - samples/sec: 124.06\n",
            "2019-11-07 00:27:03,878 epoch 7 - iter 84/146 - loss 2.44108016 - samples/sec: 119.75\n",
            "2019-11-07 00:27:07,417 epoch 7 - iter 98/146 - loss 2.42871727 - samples/sec: 128.34\n",
            "2019-11-07 00:27:10,926 epoch 7 - iter 112/146 - loss 2.39548079 - samples/sec: 129.07\n",
            "2019-11-07 00:27:14,442 epoch 7 - iter 126/146 - loss 2.38743477 - samples/sec: 129.03\n",
            "2019-11-07 00:27:18,035 epoch 7 - iter 140/146 - loss 2.38376238 - samples/sec: 126.18\n",
            "2019-11-07 00:27:19,120 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:27:19,122 EPOCH 7 done: loss 2.3908 - lr 0.1000\n",
            "2019-11-07 00:27:20,091 DEV : loss 2.3613078594207764 - score 0.8042\n",
            "2019-11-07 00:27:20,111 BAD EPOCHS (no improvement): 0\n",
            "2019-11-07 00:27:22,918 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:27:23,180 epoch 8 - iter 0/146 - loss 2.24491405 - samples/sec: 1729.21\n",
            "2019-11-07 00:27:26,959 epoch 8 - iter 14/146 - loss 2.40915984 - samples/sec: 129.32\n",
            "2019-11-07 00:27:30,786 epoch 8 - iter 28/146 - loss 2.15624259 - samples/sec: 118.34\n",
            "2019-11-07 00:27:34,243 epoch 8 - iter 42/146 - loss 2.16892943 - samples/sec: 131.20\n",
            "2019-11-07 00:27:37,877 epoch 8 - iter 56/146 - loss 2.18918517 - samples/sec: 124.69\n",
            "2019-11-07 00:27:41,377 epoch 8 - iter 70/146 - loss 2.21609910 - samples/sec: 129.71\n",
            "2019-11-07 00:27:45,060 epoch 8 - iter 84/146 - loss 2.22181527 - samples/sec: 123.26\n",
            "2019-11-07 00:27:48,357 epoch 8 - iter 98/146 - loss 2.17991538 - samples/sec: 137.51\n",
            "2019-11-07 00:27:52,193 epoch 8 - iter 112/146 - loss 2.17916282 - samples/sec: 118.06\n",
            "2019-11-07 00:27:55,742 epoch 8 - iter 126/146 - loss 2.21765622 - samples/sec: 127.71\n",
            "2019-11-07 00:27:59,547 epoch 8 - iter 140/146 - loss 2.21998238 - samples/sec: 119.01\n",
            "2019-11-07 00:28:00,684 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:28:00,686 EPOCH 8 done: loss 2.2099 - lr 0.1000\n",
            "2019-11-07 00:28:01,685 DEV : loss 1.9298806190490723 - score 0.8084\n",
            "2019-11-07 00:28:01,707 BAD EPOCHS (no improvement): 0\n",
            "2019-11-07 00:28:04,513 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:28:04,817 epoch 9 - iter 0/146 - loss 1.73597264 - samples/sec: 1544.81\n",
            "2019-11-07 00:28:08,760 epoch 9 - iter 14/146 - loss 2.02396462 - samples/sec: 121.71\n",
            "2019-11-07 00:28:12,310 epoch 9 - iter 28/146 - loss 2.03805511 - samples/sec: 127.67\n",
            "2019-11-07 00:28:15,793 epoch 9 - iter 42/146 - loss 2.09345096 - samples/sec: 130.10\n",
            "2019-11-07 00:28:19,370 epoch 9 - iter 56/146 - loss 2.08920172 - samples/sec: 126.64\n",
            "2019-11-07 00:28:22,946 epoch 9 - iter 70/146 - loss 2.04630885 - samples/sec: 126.71\n",
            "2019-11-07 00:28:26,614 epoch 9 - iter 84/146 - loss 2.08186901 - samples/sec: 123.55\n",
            "2019-11-07 00:28:30,218 epoch 9 - iter 98/146 - loss 2.02619390 - samples/sec: 125.56\n",
            "2019-11-07 00:28:33,788 epoch 9 - iter 112/146 - loss 2.03215488 - samples/sec: 126.77\n",
            "2019-11-07 00:28:38,024 epoch 9 - iter 126/146 - loss 2.02236035 - samples/sec: 107.07\n",
            "2019-11-07 00:28:41,578 epoch 9 - iter 140/146 - loss 2.03124604 - samples/sec: 127.45\n",
            "2019-11-07 00:28:42,668 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:28:42,669 EPOCH 9 done: loss 2.0309 - lr 0.1000\n",
            "2019-11-07 00:28:43,623 DEV : loss 2.1967945098876953 - score 0.7947\n",
            "2019-11-07 00:28:43,645 BAD EPOCHS (no improvement): 1\n",
            "2019-11-07 00:28:43,646 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:28:43,915 epoch 10 - iter 0/146 - loss 1.80688024 - samples/sec: 1681.47\n",
            "2019-11-07 00:28:47,662 epoch 10 - iter 14/146 - loss 1.87954365 - samples/sec: 121.01\n",
            "2019-11-07 00:28:51,350 epoch 10 - iter 28/146 - loss 1.84347752 - samples/sec: 122.75\n",
            "2019-11-07 00:28:55,071 epoch 10 - iter 42/146 - loss 1.89174545 - samples/sec: 121.62\n",
            "2019-11-07 00:28:59,210 epoch 10 - iter 56/146 - loss 1.90268598 - samples/sec: 109.31\n",
            "2019-11-07 00:29:02,685 epoch 10 - iter 70/146 - loss 1.88459653 - samples/sec: 130.32\n",
            "2019-11-07 00:29:06,167 epoch 10 - iter 84/146 - loss 1.88545705 - samples/sec: 130.08\n",
            "2019-11-07 00:29:09,861 epoch 10 - iter 98/146 - loss 1.87903084 - samples/sec: 122.77\n",
            "2019-11-07 00:29:13,533 epoch 10 - iter 112/146 - loss 1.85754338 - samples/sec: 123.28\n",
            "2019-11-07 00:29:17,277 epoch 10 - iter 126/146 - loss 1.87484029 - samples/sec: 120.86\n",
            "2019-11-07 00:29:20,699 epoch 10 - iter 140/146 - loss 1.87961910 - samples/sec: 132.45\n",
            "2019-11-07 00:29:21,910 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:29:21,912 EPOCH 10 done: loss 1.9205 - lr 0.1000\n",
            "2019-11-07 00:29:22,846 DEV : loss 2.388322114944458 - score 0.7841\n",
            "2019-11-07 00:29:22,873 BAD EPOCHS (no improvement): 2\n",
            "2019-11-07 00:29:25,610 ----------------------------------------------------------------------------------------------------\n",
            "2019-11-07 00:29:25,612 Testing using best model ...\n",
            "2019-11-07 00:29:25,614 loading file resources/taggers/example-ner/best-model.pt\n",
            "2019-11-07 00:29:36,799 0.8633\t0.7949\t0.8277\n",
            "2019-11-07 00:29:36,800 \n",
            "MICRO_AVG: acc 0.706 - f1-score 0.8277\n",
            "MACRO_AVG: acc 0.5733 - f1-score 0.6733176470588237\n",
            "M-artifact tp: 25 - fp: 10 - fn: 9 - tn: 25 - precision: 0.7143 - recall: 0.7353 - accuracy: 0.5682 - f1-score: 0.7246\n",
            "M-date     tp: 76 - fp: 3 - fn: 1 - tn: 76 - precision: 0.9620 - recall: 0.9870 - accuracy: 0.9500 - f1-score: 0.9743\n",
            "M-location tp: 10 - fp: 2 - fn: 0 - tn: 10 - precision: 0.8333 - recall: 1.0000 - accuracy: 0.8333 - f1-score: 0.9091\n",
            "M-optional tp: 0 - fp: 0 - fn: 5 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "M-organization tp: 24 - fp: 2 - fn: 12 - tn: 24 - precision: 0.9231 - recall: 0.6667 - accuracy: 0.6316 - f1-score: 0.7742\n",
            "M-percent  tp: 0 - fp: 0 - fn: 2 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
            "M-person   tp: 20 - fp: 0 - fn: 1 - tn: 20 - precision: 1.0000 - recall: 0.9524 - accuracy: 0.9524 - f1-score: 0.9756\n",
            "M-time     tp: 4 - fp: 0 - fn: 4 - tn: 4 - precision: 1.0000 - recall: 0.5000 - accuracy: 0.5000 - f1-score: 0.6667\n",
            "artifact   tp: 40 - fp: 17 - fn: 26 - tn: 40 - precision: 0.7018 - recall: 0.6061 - accuracy: 0.4819 - f1-score: 0.6504\n",
            "date       tp: 116 - fp: 11 - fn: 6 - tn: 116 - precision: 0.9134 - recall: 0.9508 - accuracy: 0.8722 - f1-score: 0.9317\n",
            "location   tp: 123 - fp: 19 - fn: 15 - tn: 123 - precision: 0.8662 - recall: 0.8913 - accuracy: 0.7834 - f1-score: 0.8786\n",
            "money      tp: 3 - fp: 1 - fn: 0 - tn: 3 - precision: 0.7500 - recall: 1.0000 - accuracy: 0.7500 - f1-score: 0.8571\n",
            "optional   tp: 2 - fp: 1 - fn: 17 - tn: 2 - precision: 0.6667 - recall: 0.1053 - accuracy: 0.1000 - f1-score: 0.1819\n",
            "organization tp: 47 - fp: 9 - fn: 29 - tn: 47 - precision: 0.8393 - recall: 0.6184 - accuracy: 0.5529 - f1-score: 0.7121\n",
            "percent    tp: 3 - fp: 2 - fn: 1 - tn: 3 - precision: 0.6000 - recall: 0.7500 - accuracy: 0.5000 - f1-score: 0.6667\n",
            "person     tp: 60 - fp: 8 - fn: 13 - tn: 60 - precision: 0.8824 - recall: 0.8219 - accuracy: 0.7407 - f1-score: 0.8511\n",
            "time       tp: 9 - fp: 4 - fn: 4 - tn: 9 - precision: 0.6923 - recall: 0.6923 - accuracy: 0.5294 - f1-score: 0.6923\n",
            "2019-11-07 00:29:36,801 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.8277,\n",
              " 'dev_score_history': [0.6969,\n",
              "  0.7686,\n",
              "  0.7586,\n",
              "  0.7531,\n",
              "  0.7721,\n",
              "  0.8007,\n",
              "  0.8042,\n",
              "  0.8084,\n",
              "  0.7947,\n",
              "  0.7841],\n",
              " 'train_loss_history': [11.781115695221784,\n",
              "  5.0517017335107886,\n",
              "  3.9207628981707847,\n",
              "  3.3375634175457365,\n",
              "  2.93023253875236,\n",
              "  2.6282880371564055,\n",
              "  2.3907947907709097,\n",
              "  2.2098855572204066,\n",
              "  2.0308768520616507,\n",
              "  1.9204641523426527],\n",
              " 'dev_loss_history': [tensor(5.6763, device='cuda:0'),\n",
              "  tensor(3.9821, device='cuda:0'),\n",
              "  tensor(2.9844, device='cuda:0'),\n",
              "  tensor(2.7174, device='cuda:0'),\n",
              "  tensor(2.5273, device='cuda:0'),\n",
              "  tensor(2.4381, device='cuda:0'),\n",
              "  tensor(2.3613, device='cuda:0'),\n",
              "  tensor(1.9299, device='cuda:0'),\n",
              "  tensor(2.1968, device='cuda:0'),\n",
              "  tensor(2.3883, device='cuda:0')]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lY8N28PVGj4",
        "colab_type": "text"
      },
      "source": [
        "# 8.固有表現抽出モデルの評価 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LsvsnR1ZUdv",
        "colab_type": "code",
        "outputId": "a2a24494-0191-47e0-f2f3-f171d6cb980c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "\n",
        "\n",
        "fn_in_test = \"data/kwdlc.test\"\n",
        "test_X, test_Y = nagisa.utils.load_file(fn_in_test, delimiter=' ', newline=\"\")\n",
        "\n",
        "true_Y = []\n",
        "pred_Y = []\n",
        "for x, true_y in zip(test_X, test_Y):\n",
        "    text = \" \".join(x)\n",
        "    sentence = Sentence(text)\n",
        "\n",
        "    model.predict(sentence)\n",
        "    tagged_text = sentence.to_tagged_string()\n",
        "\n",
        "    tokens = tagged_text.split()\n",
        "\n",
        "    words = []\n",
        "    tags = []\n",
        "    for token in tokens:\n",
        "        first_char = token[0]\n",
        "        last_char = token[-1]\n",
        "\n",
        "        if first_char == \"<\" and last_char == \">\":\n",
        "            tag = token[1:-1]\n",
        "            tags[-1] = tag\n",
        "        else:\n",
        "            words.append(token)\n",
        "            tags.append(\"O\")\n",
        "\n",
        "    pred_y = tags\n",
        "\n",
        "    true_Y += true_y\n",
        "    pred_Y += pred_y\n",
        "\n",
        "report = classification_report(true_Y, pred_Y)\n",
        "print(report)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:30:16,051 loading file resources/taggers/example-ner/final-model.pt\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    artifact       0.64      0.61      0.62        46\n",
            "        date       0.87      0.94      0.91        86\n",
            "        time       0.44      0.70      0.54        10\n",
            "    location       0.89      0.87      0.88       132\n",
            "organization       0.71      0.59      0.65        54\n",
            "      person       0.85      0.79      0.82        58\n",
            "    optional       0.36      0.27      0.31        15\n",
            "       money       0.75      1.00      0.86         3\n",
            "     percent       0.25      0.33      0.29         3\n",
            "\n",
            "   micro avg       0.79      0.78      0.79       407\n",
            "   macro avg       0.79      0.78      0.78       407\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoViwfoaVk7W",
        "colab_type": "text"
      },
      "source": [
        "# 9.固有表現抽出モデルの予測 (FLAIR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Oo5XRiV-F3",
        "colab_type": "code",
        "outputId": "a61350f1-db11-4ad7-c64e-25745177514d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model = SequenceTagger.load('resources/taggers/example-ner/final-model.pt')\n",
        "                                                                                                                                                    \n",
        "text = \"Facebook の AI ラボ 所長 でも ある ヤン ・ ルカン 博士\"         \n",
        "sentence = Sentence(text)                                               \n",
        "model.predict(sentence)                                                 \n",
        "print(sentence.to_tagged_string())  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-07 00:30:56,475 loading file resources/taggers/example-ner/final-model.pt\n",
            "Facebook の AI ラボ 所長 でも ある ヤン <B-person> ・ <M-person> ルカン <E-person> 博士\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}